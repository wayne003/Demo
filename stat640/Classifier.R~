# Initial Setup -----------------------------------------------------------
# Libraries Needed
library(randomForest)
library(plyr)
library(MASS)
library(ipred)
library(C50)
library(gbm)
library(e1071)
library(Hmisc)
library(caret)
library(progress)

# Load the Respective Data and Scale it 
setwd("/scratch/zz38/")

# Help Files
Freq_names <- read.csv("freq_names.txt", header=FALSE)
colnames(Freq_names) <- c("Freq.Name","Lower.Bound","Upper.Bound")
Freq_names$Freq.Name <- as.character(Freq_names$Freq.Name) 
Freq_names$Freq.Name[5] <- "Low_Gamma"
Freq_names$Freq.Name[6] <- "High_Gamma"

Nodes <- read.table("electrode_names.txt", quote="\"", comment.char="")
colnames(Nodes) <- c("Name")
Nodes$Name <- as.character(Nodes$Name)

# Training Data
Ytr <- read.csv('train_Y_ecog.csv',header=FALSE)
colnames(Ytr) <- c(paste0("Y",1:32))

Xtr <- read.csv('train_X_ecog.csv',header=FALSE)
colnames(Xtr) <- c(paste0("X",1:420,"_",Nodes$Name,"_",
                          rep(Freq_names$Freq.Name,each=70)))

Y_phase <- read.csv('train_Y_phase_ecog.csv',header=FALSE)
colnames(Y_phase) <- c(paste0("YP",1:32))

Tr_breaks <- read.table("train_breakpoints.txt", quote="\"", comment.char="")
Tr_breaks$V2 <- 0;Tr_breaks[1,2] <- 1 
for (i in 2:140){
  Tr_breaks[i,2] <- Tr_breaks[i-1,1]+1  
}

Tr_breaks$V3 <- 1:140;Tr_breaks <- Tr_breaks[,c(3,2,1)]
colnames(Tr_breaks) <- c("Sentence","Starting.Point","Ending.Point")

Tr_Sentences <- read.table("train_sentences.txt", quote="\"", comment.char="",
                           stringsAsFactors=FALSE)
Tr_Sentences$V6 <- 1:140
colnames(Tr_Sentences) <- c("Word1","Word2","Word3","Word4","Word5","Sentence")

# Test Data
Xtst <- read.csv('test_X_ecog.csv',header=FALSE)
colnames(Xtst) <- c(paste0("X",1:420))

Ts_breaks <- read.table("test_breakpoints.txt", quote="\"", comment.char="")
Ts_breaks$V2 <- 0;Ts_breaks[1,2] <- 1 
for (i in 2:70){
  Ts_breaks[i,2] <- Ts_breaks[i-1,1]+1  
}

Ts_breaks$V3 <- 1:70;Ts_breaks <- Ts_breaks[,c(3,2,1)]
colnames(Ts_breaks) <- c("Sentence","Starting.Point","Ending.Point")

#Crate One DataFrame to Hold all the information
MyData <- cbind(Ytr,Xtr)
MyData$Sentence = 0
# Assign Sentence # in My Data
for (i in 1:140){
  MyData[Tr_breaks$Starting.Point[i]:Tr_breaks$Ending.Point[i],]$Sentence=i
}
# Assign the Respective Words to Each Sentence
MyData=merge(x=MyData,y=Tr_Sentences,by="Sentence",all.x = TRUE)





## Test dataset
MyDataTST <- cbind(Xtst)
MyDataTST$Sentence = 0
for (i in 1:140){
  MyDataTST[Ts_breaks$Starting.Point[i]:Ts_breaks$Ending.Point[i],]$Sentence=i
}



# Classifier -----------------------------------------------------------
# X's to Select
Thesholdten=c(14,84,154,180,223,278,293,347,362,363,364,372,373,405)
Thesholdfive=c(3,4,6,9,10,11,12,14,15,16,21,22,24,25,26,29,33,34,36,40,41,48,55,56,57,59,70,72,73,76,78,79,80,81,82,84,85,86,92,93,94,97,99,101,104,105,106,109
               ,111,122,123,125,129,137,138,139,143,144,148,149,150,151,153,154,155,156,162,163,164,169,171,173,174,175,176,179,180,195,199,205,206,207,208,209
               ,210,211,212,213,214,215,216,217,218,219,221,223,224,226,231,232,233,234,239,240,241,243,244,246,249,250,265,275,276,277,278,279,280,289,291,293
               ,296,302,307,313,314,318,319,328,335,341,345,346,347,348,349,350,354,355,358,362,363,364,365,371,372,373,381,385,390,401,403,404,405,407,409,410
               ,411,413)

# Word 1 Optimum Value
WordOpt=as.data.frame(matrix(0,ncol=6,nrow=100))
colnames(WordOpt)=c("Percent","Error1","Error2","Error3","Error4","Error5")
#Progress Bar
#pb <- progress_bar$new(total = 140)
#for (l in 1:100){
#  pb$tick()
#  Sys.sleep(1 / 140)
# Subset the Data for Classifier 
MyData2=MyData[,c(Thesholdten+33,1)] # Xtr
MyDataTST2=MyDataTST[,c(Thesholdten+1,421)]

# MyData2=MyData[,c(1,
# 2:33,
# 454:458)] # Y


# Restart the Loop here
Result=as.data.frame(matrix(0,nrow=140,ncol=2))
colnames(Result)=c("Sentence","Metric")

Result$Sentence=1:140
CorResult <- as.data.frame(matrix(0,ncol=6,nrow=70))
colnames(CorResult) <- c("TestSentence","S1","S2","S3","S4","S5")

for (k in 1:70){
    print(paste("Sentences:",k))
    Xts=subset(MyDataTST2,MyDataTST2$Sentence==k)# Sentence to Predict
                                        # Xts=apply(Xts[,2:33],1,mean) # Y
    Xts=apply(Xts[,2:15],1,mean) # X




    for (i in 1:140){
        Xtr=subset(MyData2,MyData2$Sentence==i)
                                        # Xtr=apply(Xtr[,2:33],1,mean) # Y
        Xtr=apply(Xtr[,2:15],1,mean) # X


        Shortest = min(length(Xtr),length(Xts))
        Xtra=head(Xtr,Shortest)
        Xtsa=head(Xts,Shortest)  

                                        #Xtra=head(Xtr,round(length(Xtr)*1,0))
                                        #Xtsa=head(Xts,round(length(Xts)*(l/100),0))  

        Result[i,2]=max(ccf(Xtra,Xtsa,plot = FALSE,type="correlation")$acf)
    }
    

                                        #Remove the Sentence to be Predicted
    Result1=subset(Result,Result$Sentence!=k)     
                                        #Result1=Result     
    Result1=Result1[order(Result1$Metric,decreasing = TRUE),]
    Result2=Result1[1:5,]
    CorResult[k,1] <- k
    CorResult[k,2:6] <- t(Result2[,1])
}
save(CorResult,file='/scratch/zz38/CorResult.Rdata')
